{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghyL550iHBxq",
    "outputId": "5b8a3f58-25b0-4b7c-da76-3bbec597d030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully.\n",
      "Data saved to 'auto-mpg.hdf5'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "\n",
    "# Create the 'data/' directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download data using requests\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Data downloaded successfully.\")\n",
    "\n",
    "    data_content = response.text\n",
    "    df = pd.read_csv(StringIO(data_content), header=None, sep=\"\\s+\", na_values=\"?\", names=[\n",
    "        \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
    "        \"acceleration\", \"model year\", \"origin\", \"carname\"])\n",
    "\n",
    "    df = df.dropna().drop(\"carname\", axis=1)\n",
    "    input_df = df.drop(\"mpg\", axis=1)\n",
    "    output_df = df[[\"mpg\"]]\n",
    "\n",
    "    mask = np.random.rand(len(df)) < 0.8\n",
    "    train_input = input_df[mask].values\n",
    "    train_output = output_df[mask].values\n",
    "    test_input = input_df[~mask].values\n",
    "    test_output = output_df[~mask].values\n",
    "\n",
    "    with h5py.File(os.path.join(data_dir, 'auto-mpg.hdf5'), 'w') as f:\n",
    "        train = f.create_group(\"train\")\n",
    "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input)\n",
    "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output)\n",
    "\n",
    "        test = f.create_group(\"test\")\n",
    "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input)\n",
    "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output)\n",
    "\n",
    "    print(\"Data saved to 'auto-mpg.hdf5'.\")\n",
    "else:\n",
    "    print(f\"Failed to download data. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkS1453ie8Mb",
    "outputId": "b692c1cc-cfd1-4f18-c8e8-96e2d57ac4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully.\n",
      "Data saved to: data/uci-har.hdf5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Specify the URL for the UCI HAR Dataset.zip file\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
    "\n",
    "\n",
    "# Download and extract the dataset\n",
    "with zipfile.ZipFile(io.BytesIO(urllib.request.urlopen(url).read()), 'r') as zip:\n",
    "    train_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/train/X_train.txt\"))\n",
    "    train_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/train/y_train.txt\")))\n",
    "    test_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/test/X_test.txt\"))\n",
    "    test_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/test/y_test.txt\")))\n",
    "\n",
    "# Create an HDF5 file to store the data\n",
    "hdf5_path = 'data/uci-har.hdf5'\n",
    "with h5py.File(hdf5_path, 'w') as f:\n",
    "    train = f.create_group(\"train\")\n",
    "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype=np.dtype(\"f2\"))\n",
    "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype=np.dtype(\"i1\"))\n",
    "    test = f.create_group(\"test\")\n",
    "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype=np.dtype(\"f2\"))\n",
    "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype=np.dtype(\"i1\"))\n",
    "\n",
    "# Print a message indicating that the data has been downloaded and saved\n",
    "print(\"Data downloaded successfully.\")\n",
    "print(\"Data saved to:\", hdf5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46ZpC_I3g4t0",
    "outputId": "b41d4b4c-aabb-4faf-9e0e-1c581b875057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully.\n",
      "Training data shape: (24172, 104), (24172, 1)\n",
      "Testing data shape: (5990, 104), (5990, 1)\n",
      "Data saved to 'data/income.hdf5'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "column_names = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
    "    \"hours_per_week\", \"native_country\", \"income\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(url, header=None, sep=r\"\\s*,\\s*\", na_values=\"?\", engine=\"python\", names=column_names)\n",
    "    df = df.dropna()\n",
    "    df = pd.get_dummies(df)\n",
    "    df = df.drop(\"income_<=50K\", axis=1)\n",
    "    input_df = df.drop(\"income_>50K\", axis=1)\n",
    "    output_df = df[[\"income_>50K\"]]\n",
    "\n",
    "    mask = np.random.rand(len(df)) < 0.8\n",
    "    train_input = input_df[mask].values.astype(np.float32)  # Convert to float32\n",
    "    train_output = output_df[mask].values.astype(np.int32)   # Convert to int32\n",
    "    test_input = input_df[~mask].values.astype(np.float32)   # Convert to float32\n",
    "    test_output = output_df[~mask].values.astype(np.int32)    # Convert to int32\n",
    "\n",
    "    output_dir = 'data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir, 'income.hdf5')\n",
    "\n",
    "    with h5py.File(file_path, 'w') as f:\n",
    "        train = f.create_group(\"train\")\n",
    "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype='f')\n",
    "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype='i')\n",
    "\n",
    "        test = f.create_group(\"test\")\n",
    "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype='f')\n",
    "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype='i')\n",
    "\n",
    "    print(\"Data downloaded successfully.\")\n",
    "    print(f\"Training data shape: {train_input.shape}, {train_output.shape}\")\n",
    "    print(f\"Testing data shape: {test_input.shape}, {test_output.shape}\")\n",
    "    print(f\"Data saved to '{file_path}'.\")\n",
    "\n",
    "except HTTPError as e:\n",
    "    print(f\"An error occurred while trying to download the data: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w8j_wJxiii5l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pytest\n",
    "import tensorflow\n",
    "\n",
    "import nn\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def set_seeds():\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    tensorflow.random.set_seed(42)\n",
    "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "\n",
    "def test_deep_vs_wide(capsys):\n",
    "    train_in, train_out, test_in, test_out = load_hdf5(\"data/auto-mpg.hdf5\")\n",
    "\n",
    "    deep, wide = nn.create_auto_mpg_deep_and_wide_networks(\n",
    "        train_in.shape[-1], train_out.shape[-1])\n",
    "\n",
    "    # check that the deep neural network is indeed deeper\n",
    "    assert len(deep.layers) > len(wide.layers)\n",
    "\n",
    "    # check that the 2 networks have (nearly) the same number of parameters\n",
    "    params1 = deep.count_params()\n",
    "    params2 = wide.count_params()\n",
    "    assert abs(params1 - params2) / (params1 + params2) < 0.05\n",
    "\n",
    "    # check that the 2 networks have the same compile parameters\n",
    "    assert_compile_parameters_equal(deep, wide)\n",
    "\n",
    "    # check that the 2 networks have the same activation functions\n",
    "    assert set(hidden_activations(deep)) == set(hidden_activations(wide))\n",
    "\n",
    "    # check that output type and loss are appropriate for regression\n",
    "    assert all(\"mean\" in loss_name(model) for model in [deep, wide])\n",
    "    assert loss_name(deep) == loss_name(wide)\n",
    "    assert output_activation(deep) == output_activation(wide) == \\\n",
    "        tensorflow.keras.activations.linear\n",
    "\n",
    "    # train both networks\n",
    "    deep.fit(train_in, train_out, verbose=0, epochs=100)\n",
    "    wide.fit(train_in, train_out, verbose=0, epochs=100)\n",
    "\n",
    "    # check that error level is acceptable\n",
    "    mean_predict = np.full(shape=test_out.shape, fill_value=np.mean(train_out))\n",
    "    [baseline_rmse] = root_mean_squared_error(mean_predict, test_out)\n",
    "    [deep_rmse] = root_mean_squared_error(deep.predict(test_in), test_out)\n",
    "    [wide_rmse] = root_mean_squared_error(wide.predict(test_in), test_out)\n",
    "    with capsys.disabled():\n",
    "        rmse_format = \"{1:.1f} RMSE for {0} on Auto MPG\".format\n",
    "        print()\n",
    "        print(rmse_format(\"baseline\", baseline_rmse))\n",
    "        print(rmse_format(\"deep\", deep_rmse))\n",
    "        print(rmse_format(\"wide\", wide_rmse))\n",
    "\n",
    "    assert deep_rmse < baseline_rmse\n",
    "    assert wide_rmse < baseline_rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_dropout(capsys):\n",
    "\n",
    "    train_in, train_out, test_in, test_out = load_hdf5(\"data/uci-har.hdf5\")\n",
    "\n",
    "    # keep only every 10th training example\n",
    "    train_out = train_out[::10, :]\n",
    "    train_in = train_in[::10, :]\n",
    "\n",
    "    drop, no_drop = nn.create_activity_dropout_and_nodropout_networks(\n",
    "        train_in.shape[-1], train_out.shape[-1])\n",
    "\n",
    "    # check that the dropout network has Dropout and the other doesn't\n",
    "    assert any(isinstance(layer, tensorflow.keras.layers.Dropout)\n",
    "               for layer in drop.layers)\n",
    "    assert all(not isinstance(layer, tensorflow.keras.layers.Dropout)\n",
    "               for layer in no_drop.layers)\n",
    "\n",
    "    # check that the 2 networks have the same number of parameters\n",
    "    assert drop.count_params() == no_drop.count_params()\n",
    "\n",
    "    # check that the two networks are identical other than dropout\n",
    "    dropped_dropout = [l for l in drop.layers\n",
    "                       if not isinstance(l, tensorflow.keras.layers.Dropout)]\n",
    "    assert_layers_equal(dropped_dropout, no_drop.layers)\n",
    "\n",
    "    # check that the 2 networks have the same compile parameters\n",
    "    assert_compile_parameters_equal(drop, no_drop)\n",
    "\n",
    "    # check that output type and loss are appropriate for multi-class\n",
    "    assert all(\"categorical\" in loss_name(model)\n",
    "               for model in [drop, no_drop])\n",
    "    assert loss_name(drop) == loss_name(no_drop)\n",
    "    assert output_activation(drop) == output_activation(no_drop) == \\\n",
    "        tensorflow.keras.activations.softmax\n",
    "\n",
    "    # train both networks\n",
    "    drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
    "    no_drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
    "\n",
    "    # check that accuracy level is acceptable\n",
    "    baseline_prediction = np.zeros_like(test_out)\n",
    "    baseline_prediction[:, np.argmax(np.sum(train_out, axis=0), axis=0)] = 1\n",
    "    baseline_accuracy = multi_class_accuracy(baseline_prediction, test_out)\n",
    "    dropout_accuracy = multi_class_accuracy(drop.predict(test_in), test_out)\n",
    "    no_dropout_accuracy = multi_class_accuracy(\n",
    "        no_drop.predict(test_in), test_out)\n",
    "    with capsys.disabled():\n",
    "        accuracy_format = \"{1:.1%} accuracy for {0} on UCI-HAR\".format\n",
    "        print()\n",
    "        print(accuracy_format(\"baseline\", baseline_accuracy))\n",
    "        print(accuracy_format(\"dropout\", dropout_accuracy))\n",
    "        print(accuracy_format(\"no dropout\", no_dropout_accuracy))\n",
    "    assert dropout_accuracy >= 0.75\n",
    "    assert no_dropout_accuracy >= 0.75\n",
    "\n",
    "\n",
    "def test_early_stopping(capsys):\n",
    "    train_in, train_out, test_in, test_out = load_hdf5(\"data/income.hdf5\")\n",
    "\n",
    "    # Keep only every 5th training example instead of 10 for more data\n",
    "    train_out = train_out[::5, :]\n",
    "    train_in = train_in[::5, :]\n",
    "\n",
    "    early, early_fit_kwargs, late, late_fit_kwargs = \\\n",
    "        nn.create_income_earlystopping_and_noearlystopping_networks(\n",
    "            train_in.shape[-1], train_out.shape[-1])\n",
    "\n",
    "    # Check that the two networks have the same number of parameters\n",
    "    assert early.count_params() == late.count_params()\n",
    "\n",
    "    # Check that the two networks have identical layers\n",
    "    assert_layers_equal(early.layers, late.layers)\n",
    "\n",
    "    # Check that the 2 networks have the same compile parameters\n",
    "    assert_compile_parameters_equal(early, late)\n",
    "\n",
    "    # Check that output type and loss are appropriate for binary classification\n",
    "    assert all(any(x in loss_name(model) for x in {\"crossentropy\", \"hinge\"})\n",
    "               and \"categorical\" not in loss_name(model)\n",
    "               for model in [early, late])\n",
    "    assert loss_name(early) == loss_name(late)\n",
    "    assert output_activation(early) == output_activation(late) == \\\n",
    "        tensorflow.keras.activations.sigmoid\n",
    "\n",
    "    # Train both networks with increased epochs and a validation set for early stopping\n",
    "    early_fit_kwargs.update(verbose=0, epochs=100, validation_data=(test_in, test_out))\n",
    "    early_hist = early.fit(train_in, train_out, **early_fit_kwargs)\n",
    "\n",
    "    late_fit_kwargs.update(verbose=0, epochs=100)  # More epochs for late\n",
    "    late_hist = late.fit(train_in, train_out, **late_fit_kwargs)\n",
    "\n",
    "    # Check that accuracy levels are acceptable\n",
    "    all1_accuracy = np.sum(test_out == 1) / test_out.size\n",
    "    early_accuracy = binary_accuracy(early.predict(test_in), test_out)\n",
    "    late_accuracy = binary_accuracy(late.predict(test_in), test_out)\n",
    "\n",
    "    with capsys.disabled():\n",
    "        accuracy_format = \"{1:.1%} accuracy for {0} on census income\".format\n",
    "        print()\n",
    "        print(accuracy_format(\"baseline\", all1_accuracy))\n",
    "        print(accuracy_format(\"early\", early_accuracy))\n",
    "        print(accuracy_format(\"late\", late_accuracy))\n",
    "\n",
    "    assert early_accuracy > 0.75\n",
    "    assert late_accuracy > 0.75\n",
    "    assert early_accuracy > all1_accuracy\n",
    "    assert late_accuracy > all1_accuracy\n",
    "\n",
    "    # Check that the first network stopped early (fewer epochs)\n",
    "    assert len(early_hist.history[\"loss\"]) < len(late_hist.history[\"loss\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_hdf5(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        train = f[\"train\"]\n",
    "        train_out = np.array(train[\"output\"])\n",
    "        train_in = np.array(train[\"input\"])\n",
    "        test = f[\"test\"]\n",
    "        test_out = np.array(test[\"output\"])\n",
    "        test_in = np.array(test[\"input\"])\n",
    "    return train_in, train_out, test_in, test_out\n",
    "\n",
    "\n",
    "def assert_layers_equal(layers1: List[tensorflow.keras.layers.Layer],\n",
    "                        layers2: List[tensorflow.keras.layers.Layer]):\n",
    "    def layer_info(layer):\n",
    "        return (layer.__class__,\n",
    "                getattr(layer, \"units\", None),\n",
    "                getattr(layer, \"activation\", None))\n",
    "\n",
    "    assert [layer_info(l) for l in layers1] == [layer_info(l) for l in layers2]\n",
    "\n",
    "\n",
    "def assert_compile_parameters_equal(model1: tensorflow.keras.models.Model,\n",
    "                                    model2: tensorflow.keras.models.Model):\n",
    "    def to_dict(obj):\n",
    "        items = dict(__class__=obj.__class__.__name__, **vars(obj))\n",
    "        to_remove = {key for key, value in items.items() if key.endswith(\"_fn\")}\n",
    "        for key in to_remove:\n",
    "            items.pop(key)\n",
    "\n",
    "    assert to_dict(model1.optimizer) == to_dict(model2.optimizer)\n",
    "\n",
    "\n",
    "def loss_name(model):\n",
    "    if isinstance(model.loss, str):\n",
    "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
    "    else:\n",
    "        loss = model.loss\n",
    "    return loss.__name__.lower()\n",
    "\n",
    "\n",
    "def hidden_activations(model):\n",
    "    return [layer.activation\n",
    "            for layer in model.layers[:-1] if hasattr(layer, \"activation\")]\n",
    "\n",
    "\n",
    "def output_activation(model):\n",
    "    return model.layers[-1].activation\n",
    "\n",
    "\n",
    "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
    "    return ((system - human) ** 2).mean(axis=0) ** 0.5\n",
    "\n",
    "\n",
    "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
    "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
    "\n",
    "\n",
    "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
    "    return np.mean(np.round(system) == human)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
