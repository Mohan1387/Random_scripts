{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_input, train_output), (test_input, test_output) = mnist.load_data()\n",
        "\n",
        "# Expand dimensions to make compatible with CNN input shape\n",
        "train_input = np.expand_dims(train_input, axis=-1)\n",
        "test_input = np.expand_dims(test_input, axis=-1)\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "train_output = to_categorical(train_output).astype(np.float32)\n",
        "test_output = to_categorical(test_output).astype(np.float32)\n",
        "\n",
        "# Create 'data' directory if it doesn't exist\n",
        "if not os.path.exists('data'):  # For Assignment data is provided in hdf5 format\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Save preprocessed data to HDF5 file\n",
        "with h5py.File('data/mnist.hdf5', 'w') as f:\n",
        "    # Include only every 100th training/testing example to limit dataset size\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input[::100])\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output[::100])\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input[::100])\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output[::100])\n"
      ],
      "metadata": {
        "id": "AZOHMp05TVhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download .csv files from\n",
        "    #     https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
        "names = [\"1-Psy\", \"2-KatyPerry\", \"3-LMFAO\",\"4-Eminem\", \"5-Shakira\"]  # For Assignment all 5 datasets are provided\n",
        "dfs = [pd.read_csv(\"data/Youtube0{0}.csv\".format(name)) for name in names]\n",
        "tokenize = re.compile(r\"\\d+|[^\\d\\W]+|\\S\").findall\n",
        "dfs_tokenized = [[tokenize(comment) for comment in df[\"CONTENT\"]]\n",
        "                 for df in dfs]\n",
        "\n",
        "index_to_token = [''] + sorted(set(token\n",
        "                                   for comments in dfs_tokenized\n",
        "                                   for tokens in comments\n",
        "                                   for token in tokens))\n",
        "\n",
        "token_to_index = {c: i for i, c in enumerate(index_to_token)}\n",
        "\n",
        "max_tokens = max(len(tokens)\n",
        "                 for comments in dfs_tokenized\n",
        "                 for tokens in comments)\n",
        "\n",
        "with h5py.File('data/youtube-comments.hdf5', 'w') as f:\n",
        "    f.attrs[\"vocabulary\"] = json.dumps(index_to_token)\n",
        "    for name, df, comments in zip(names, dfs, dfs_tokenized):\n",
        "        matrix_in = np.zeros(shape=(len(comments), max_tokens))\n",
        "        for i, tokens in enumerate(comments):\n",
        "            for j, token in enumerate(tokens):\n",
        "                matrix_in[i, j] = token_to_index[token]\n",
        "        matrix_out = df[\"CLASS\"].values.reshape((-1, 1))\n",
        "        group = f.create_group(name)\n",
        "        group.create_dataset(\"input\", compression=\"gzip\", data=matrix_in)\n",
        "        group.create_dataset(\"output\", compression=\"gzip\", data=matrix_out)\n"
      ],
      "metadata": {
        "id": "RPJKXlx2e4pq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w8j_wJxiii5l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_toy_rnn(capsys):\n",
        "    n_train = 20\n",
        "    n_test = 10\n",
        "    n_timesteps = 20\n",
        "    n_features = 2\n",
        "\n",
        "    # create random input for train and test\n",
        "    train_in = np.random.randint(1, 11, (n_train, n_timesteps, n_features))\n",
        "    test_in = np.random.randint(1, 11, (n_test, n_timesteps, n_features))\n",
        "\n",
        "    # deterministically create output from the random input\n",
        "    def out(matrix_in):\n",
        "        matrix_out = np.zeros(shape=matrix_in.shape[:-1] + (1,))\n",
        "        for i, example in enumerate(matrix_in):\n",
        "            for j, [_, x1] in enumerate(example):\n",
        "                [x0, _] = example[j - 3] if j >= 3 else [0., 0.]\n",
        "                matrix_out[i, j] = x0 - x1\n",
        "        return matrix_out\n",
        "    train_out = out(train_in)\n",
        "    test_out = out(test_in)\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, _, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_toy_rnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"mean\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.linear\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=20, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure error is low enough\n",
        "    rmse = root_mean_squared_error(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1f} RMSE for RNN on toy problem\".format(rmse))\n",
        "    assert rmse < 2\n",
        "\n",
        "\n",
        "def test_image_cnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/mnist.hdf5\", 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_mnist_cnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"categorical\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.softmax\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = multi_class_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on MNIST sample\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_rnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_rnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for RNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_cnn(capsys):\n",
        "    # The data below was obtained as in test_text_rnn\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_cnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def layers(model: tensorflow.keras.models.Model):\n",
        "    return [x.layer if isinstance(x, tensorflow.keras.layers.Wrapper) else x\n",
        "            for x in model.layers]\n",
        "\n",
        "\n",
        "def is_convolution(layer: tensorflow.keras.layers.Layer):\n",
        "    return layer.__class__.__name__.startswith('Conv')\n",
        "\n",
        "\n",
        "def is_recurrent(layer: tensorflow.keras.layers.Layer):\n",
        "    return isinstance(layer, tensorflow.keras.layers.RNN)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def output_activation(model: tensorflow.keras.models.Model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean() ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}